# **IV.	ADDENDUM: CRITICAL ISSUES** Below is an expanded discussion on the five issues that resonated as critical takeaways for further study and for potential future Roundtables dedicated to each subject area.  **A.	Philosophy: FOIA vs. Open Data**Under current practices, many government entities interpret the Open Data movement within the traditional rubric of the Freedom of Information Act (FOIA).  FOIA and the Open Data philosophy, however, provide alternative frameworks about how data is treated. As a result, conflicts often emerge from a clash of cultures that lead to divergent policies, inconsistent practices, and frustrations by all parties. Though perhaps oversimplified, traditional FOIA models function as a default closed framework; data is provided when asked. By contrast contemporary Open Data models function as a default open framework; data is published automatically and regularly. The Sunlight Foundation captures this distinction by referring to the traditional government disclosure laws as reactive, while the Open Data frameworks are described as “[p]roactive disclosure [and] the release of public information before an individual requests it. In the 21st century that means proactively putting new information online, where people are looking for it.”  Local governments, however, are still mostly locked in the traditional framework. And moving to the Open Data framework is not easy because of the costs involved. So it’s not surprising that local governments tend to adopt Open Data frameworks in fits and starts. In turn, this “two-steps forward and one-step back” approach leads to confusion because the differing frameworks create uncertainty amongst government officials, citizens, companies, etc. 1/	*Traditional paradigm*Historically, FOIA requests have been the method by which the government has approached the dissemination of data for use by the public. The FOIA paradigm focuses on data as means to prevent fraud or to gain insight into hidden government activities and decisions. This overarching paradigm thus tends to lead government entities to interpret Open Data policies as a compliance regime, which in turn implies additional costs on already restricted resources. This framework has two consequences for government actors. First, FOIA requests are a cost center, both in labor and money, outside of normal operations. And second, traditional stakeholders of the FOIA regime, such as journalists when submitting FOIA requests, are often seen as at odds with government agencies. Consequently, FOIA regimes are commonly perceived as more nuisance than benefit for government actors.This perspective is mirrored in State law; for example, in California, policymakers think of Open Data largely in terms of compliance with the California Public Records Act,  the state’s analogue to FOIA. This overarching paradigm thus tends to lead government entities to interpret Open Data policies as a compliance regime, which in turn implies additional costs on already restricted resources. 2/	*Open Data*By contrast, many Open Data advocates have a data-driven approach. The Open Data movement views government data as a means to improve services, decrease costs, and open up new economic opportunities for local businesses. Data that the government has collected, stored, and indexed is viewed as a doorway of opportunity that can be leveraged for economic purposes, to improve the data the government uses to make more informed decision-making, and to make government more accessible and transparent. The premise for this approach is the presumption that much of the data sought is relatively innocuous information, such as logistical bus maintenance schedules. 	3/	ConflictYet even such seemingly innocuous data can introduce problems for Open Data frameworks. As a data steward – the entity responsible for establishing data standards, access security requirements, and general data management – city government will be required to promulgate new policies in response to duties and responsibilities that differ from the traditional FOIA request regime. Accordingly, despite some overlap, constructive discussions about Open Data policies must treat Open Data as an independent framework from the existing FOIA structure. For example, the City of Oakland received requests from a journalist to hide any searches made by that journalist on a city-managed, open data website. The journalist was concerned that other journalists might see the searches and take the story. While the City of Oakland has no duty to the journalist, the broader question of whether an individual should have the right to search privately through public documents generated serious internal discussion about Open Data policies and practices.  **Takeaway**For policymakers, in discerning between policies that are FOIA versus those that are Open Data, a critical question will be to determine: what is the purpose of releasing this data? Compliance and anti-corruption objectives will lean toward a FOIA framework, while data-driven decision-making and support in economic growth will lean toward Open Data initiatives. This is not to say that the difference is clean cut, but understanding the purpose of releasing the data will make it easier to establish a constructive conversation about necessary policies and whether stakeholders are seeking a default open or default closed framework. **B.	Policy: Data Classification**All data is not the same. Government entities collect substantial amounts of personally identifiable information.  To protect against the disclosure of personally identifiable data, government entities may seek to classify data into “flavors of personally identifiable information” as one Roundtable participant described the challenge. Innocuous data can be released with less review, while more personally identifiable data may need to be anonymized or aggregated before release. Attempts to categorize data, however, are not a panacea. Quite the contrary, critical challenges exist. First, personally identifiable data may be accidentally released. And once released, data is, essentially, permanently accessible. Second, information that was not understood to be personally identifiable may be disseminated, only to realize in hindsight that it should have been classified as personally identifiable. Third, advances in de-anonymization and data aggregation techniques mean any iota of personal data can be made personally identifiable to some degree or another. And it is incredibly easy to re-identify data. Consequently, once data is re-identified, it’s re-identified forever. In their role as data stewards, governments should address the aforementioned challenges, focusing on erecting processes around the collection, storage, and disclosure of data. One recommended process is to (i) identify data sources, (ii) identify data sets, and (iii) complete a dataset catalog. An aspect of this process is classifying data depending on its degree of sensitivity. Depending on the level of sensitivity, different rules apply that dictate 18 whether the information may be disclosed; when the information is disclosed, whether some aspects of the information should be removed before disclosure, and so forth. The premise is that data with explicit information about an individual is the most sensitive and valuable. Therefore, it should be treated with rigorous process.  It is important, however, to distinguish the data that government collects into two categories. The first category is data the government generates by its activities. This data includes purchases of office supplies, bus schedules, meeting times, schedule maintenance, vendor contracts, etc. Outside of a few exceptions related to public safety or health and human services, government generated data generally can be released without much concern. But data classification becomes more problematic in the second category of data collected– data collected about individuals in their interactions with government entities. Here, the seemingly obvious division between explicitly personally identifiable data and non-obvious personally identifiable data no longer exists; re-identification techniques utilizing massive existing datasets means personal data may be made personally identifiably given sufficient time, resources, and access to other public records. Moreover, it’s not always entirely clear when innocuous data (category one data) might incidentally include more sensitive data (category two data). Accordingly, policymakers must tread carefully when seeking to classify data as explicitly personally identifiable or not. The ability of any policymaker to divine what is explicitly personal and what is not is increasingly more uncertain. Technology and techniques for the de-anonymization of data is becoming more sophisticated, thereby making it easier to identify individuals. Accordingly, any express delineations of what is not personally identifiably information will likely fail in time because of rapidly evolving techniques in re-identification.These concerns are not to discount that data about an individual might already be publicly accessible in some form or another.  Rather, the concern is that malicious actors will exploit data about individuals collected and published by the government. Furthermore, advances in re-identification techniques mean that which cannot be identified today is susceptible to re-identification tomorrow. Consequently, a government policy to define some data as personally identifiable and other data as innocuous is infeasible. A more blunt way to state the problem might be: in today’s world, any data about an individual has the potential to be sensitive. Therefore, treating some data as less personally identifiable than other data is a naïve approach.**Takeaway**Assuming that data can be classified as personally identifiable or not personally identifiable might be a false start. But withholding data is not an option, at least not a long-term solution and nor should it be. Accordingly, policymakers must find a balance between making data accessible, while anticipating that misuse of data will occur. An analogy for the challenge would be to prepare against a data breach, but knowing it’s more likely to occur than not. This means that when erecting processes to make data available, parallel processes are developed to help individuals, especially from the most vulnerable populations, when data is exploited for ill purposes.  **C.	Process: Policy Setting Body & Chief Data Officer**Advances in technology coupled with the wide variety of data that government entities collect make setting rigid and enduring laws difficult. In order to keep pace with these changes, government entities must establish the internal infrastructure and expertise to set policies and develop resources. For cities, this means erecting a policy-setting body to coordinate across divisions and hire administratively defined staff, such as a chief data officer. Existing models, such as the Chief Data Officer in San Francisco provide a good model for other cities.Recognizing the importance of information technology, beyond a cost-benefit analysis, requires municipalities to erect formal structures within the decision-making processes. These formal structures should be enacted through codification of rules and include a policy-making body with duty, responsibility, and authority to manage and coordinate a city’s IT policy. No one individual or process can manage this process across all of a city’s services. San Francisco’s experience with the establishment of the Committee on Information Technology is a good example of a decade-long process that eventually influenced new administrative code, the San Francisco Open Data Law. The Open Data law defines roles and assigns clear responsibly to an internal committee with major stakeholders, authority to implement citywide policies, and serves as a method to communicate with the community. Equally important, a policy-setting body is important because it provides an authority to which full time staff can report. Establishing clear lines of authority enables administrative staff the flexibility and power to design, develop, and implement the processes and procedures to implement effective Open Data policies. For example, San Francisco’s Open Data law formalizes the role of Chief Data Officer; it also sets clear duties and responsibilities for the Chief Data Officer. And while not formal, the City of Oakland has built on its experience with the Domain Awareness Center (DAC) —turning that body into a semi-formal advisory group to inform not just the data policies around the DAC but also for Oakland’s other Open Data projects.Oakland has taken formal steps to develop the processes through Resolution No. 84659. This Resolution formalized policies that other cities can emulate, including requiring the City of Oakland to: (i) identify an Open Data liaison in each department of the city; (ii) establish clear guidelines for identifying high value and high interest city data sets and policies for organizing publication of that data; (iii) include a public engagement strategy to ensure ongoing feedback and collaboration with citizens and data users; and (iv) include city guidelines for maintaining consistency with applicable laws and best practices, including those related to privacy.Bracketing legislative efforts such as Resolution No. 84659, the complex and dynamic state of advances in technology and data management requires administrative expertise. This expertise is generally found in the Chief Data Officer (CDO) for a municipality. An internal subject matter expert, the CDO can erect processes and procedures for Open Data best practices and serve as a liaison for various stakeholders within government.  In addition, the CDO serves as a conduit between government and society, communicating government objectives, priorities, and resource constraints while also identifying and integrating the community’s response, ideas, and problems.**Takeaway**Enacting legislative code, establishing an internal policy making body, and formalizing the position of CDO may seem straightforward. All of these suggestions are consistent with common government practice and are a growing trend.  Nonetheless, these very clear steps are still incredibly difficult for most cities. This is because cities lack financial resources, making the process somewhat of a luxury even if the long-term demands make these issues more of an imperative. Adhering to legislation is costly. Hiring a qualified Chief Data Officer is costly. As the City of Oakland pointed out, the role of the Chief Information Officer is different than the CDO, and the city simply cannot afford a chief data officer. Thus, having a playbook for cities can help reduce costs for Open Data initiatives across the nation. In the absence of a CDO, the playbook would provide policymakers with an accessible resource to navigate the issues, identify areas where outside expertise might be necessary, and still promote Open Data practices.	**D.	Emergent Issue: Non-Consensual Transparency**Advocates for Open Data generally operate as part of a larger initiative to make government more accessible and transparent. But Open Data also carries challenges to the social contract – Open Data initiatives may undermine the traditional notice and consent framework because individuals are giving up personal identifiable information without choice.  Unlike a social media platform, where individuals voluntarily provide their data to use services, government authority often compels individuals to disclose personal information. This non-consensual transparency exposes entire communities – often the most vulnerable segments – to potential exploitation, with the government as facilitator.  Much of the data government entities are seeking to publish online is already public. But the breadth and depth of the data available today, combined with the accessibility of the data, introduces new concerns.  Now, more than ever, government entities are under pressure to publish nearly their entire data catalog: planning information, public health information, etc. Data previously found only in filing cabinets or proprietary digital systems is now imminently more accessible. The fear is how third parties can use this non-consensually acquired data. Third parties are not prevented from using government data for re-identification of individuals. As a general matter of public policy, this is not categorically a negative result. Society has already determined that some data is publicly accessible. The difference is that barriers to acquiring that data and its aggregation have plummeted. As a result, citizens are compelled to give up their information, but suffer the costs of potentially having personal identifiable data exposed to a wider network. More to the point, however, is that cities cannot assume that all data made accessible will be used for the purpose intended.  Just as companies may take every effort to prevent a data breach, they must prepare for the likelihood that one will occur. Data security vulnerabilities and breaches inevitably arise, despite even the best of intentions and the employment of data security best practices. Malicious use will occur. Consequently, it would be an inept policymaker that assumes all breaches can be prevented and that malicious actors might not abuse access to that data.Equal access to data means access for everyone, a lesson the City of Oakland learned, with the best of intentions. So while it might be impossible to predict what those with bad intent will do with data, it is well within a city’s ability to assume data will be used for illegal activities, targeting the most vulnerable. The failure to take that truth into account when defining data dissemination processes borders on the unethical.  The question then, is: how do cities balance society’s right to know with the risks brought on by non-consensual transparency? **Takeaway**Non-consensual transparency is a challenge that follows an Open Data society. Consequently, new practices will be required, such as developing a complaint procedure for those who are preyed on by data aggregators. Individuals must be able to find redress from the government if government disseminated data is inaccurate and leads to deleterious effects. Government agencies must be prepared to manage that process efficiently with effective results. Policymakers must also keep in mind that the most vulnerable communities, such as the elderly, children, and the impoverished, are also the most likely to be preyed upon. This introduces additional societal burdens often overlooked in the traditional cost-benefit analysis. Finally, but just as important, the ability of data to disseminate easily means that no single city will be able to solve these questions in isolation. Data (1’s and 0’s) sees no jurisdiction. Accordingly, Statewide discussions will be needed to establish consistency across entities and shared practices to respond to inevitable violations by those with ill intent. **E.	Emergent Issue: Liability Externality**Municipalities beware: Open Data initiatives may place government entities in the role of data broker, including the potential for accompanying liability. By serving as the data source to collect, store, index, and publish data, third parties will rely on the government for their businesses, and individuals are likely to expect government entities to provide clean data and processes to rectify incorrect data. This relationship creates a series of explicit and implicit contracts upon which users can reasonably rely. Consequently, governments must be cognizant of the potential liabilities that emerge when collecting, storing, and publishing data publicly.Although governments might not sell the data as commercial data brokers do, in practice their efforts to package data and make it accessible enables third parts to leverage that data in business – to target citizens as potential customers.  Policymakers, in turn, must be aware of how to manage that liability, either through internal processes, practices, and procedures, or by new models to externalize liability to third parties. This is particularly important if government policies generate reliance on the government’s actions as a data source by third party entities such as for-profit and nonprofit organizations. As a consequence, government must ensure that multiple stakeholders beyond just technologists are engaged in these conversations. These concerns are one of the critical reasons that the FOIA framework is still relied upon. FOIA requests operate within a well-understood legal regime and processes. This infrastructure limits the likelihood of any liability for issues around the publication of data. Open Data initiatives, however, are new, not well understood, and lack well known established procedures. Furthermore, Open Data initiatives operate with the implied assumption that municipalities operate as a trusted data source—in effect, a data broker. Once a municipality assumes the role of a trusted data source, it takes on additional duties and responsibilities.  The government entity will be required to ensure that data is accurate, take steps to sanitize data before publication, and construct policies and procedures around the ability to access or delete the data. Failure to do so introduces a host of potential liability concerns. Liability issues are not an unknown or new concern. There are models of federal government agencies externalizing liability in data collection from municipalities. These models build in data auditing requirements for data-providing units. For example, the U.S. Census Comprehensive Financial Annual Report requires municipalities to sign off on an audit that they do not collect personal identifiable data in the census data. This pushes some liability off the federal government to the municipality. **Takeaway**Before taking on the responsibility to serve as a regional data source, municipalities must review the additional duties that the role entails. A thorough review requires an evaluation of policy, technical architecture, the costs to implement the program, and anticipated third party use of the data. The federal government provides several resources to initiate this discourse, such as the Data.gov Policy Statements or the work by 18F out of the General Services Administration.